[mode]
model_name = mt-lstm-crf
save_model = True

[data]
dataset_folder = /home/john/dev/saber/datasets_2/BC5CDR_DISO_BIO, /home/john/dev/saber/datasets_2/Variome_DISO_BIO, /home/john/dev/saber/datasets_2/NCBI_Disease_BIO, /home/john/dev/saber/datasets_2/CRAFT_BIO, /home/john/dev/saber/datasets_2/Linnaeus_BIO, /home/john/dev/saber/datasets_2/MLEE_BIO, /home/john/dev/saber/datasets_2/AnatEM_BIO, /home/john/dev/saber/datasets_2/BC5CDR_CHED_BIO, /home/john/dev/saber/datasets_2/BC2GM_BIO
output_folder = /home/john/dev/saber/output/pretrained_DISO
pretrained_model = 
pretrained_embeddings = /home/john/dev/saber/word_embeddings/wikipedia-pubmed-and-PMC-w2v.bin

[model]
word_embed_dim = 200
char_embed_dim = 30

[training]
optimizer = nadam
activation = relu
grad_norm = 1.0
learning_rate = 0.0
decay = 0.0
dropout_rate = 0.3, 0.1, 0.3
batch_size = 32
k_folds = 20
epochs = 25
criteria = exact

[advanced]
verbose = True
debug = False
tensorboard = False
save_all_weights = True
replace_rare_tokens = True
load_all_embeddings = False
fine_tune_word_embeddings = False
variational_dropout = True

