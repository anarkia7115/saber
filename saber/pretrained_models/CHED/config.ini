[mode]
model_name = mt-lstm-crf
save_model = True

[data]
dataset_folder = /home/john/dev/saber/datasets/BC4CHEMD_BIO, /home/john/dev/saber/datasets/BC5CDR_BIO, /home/john/dev/saber/datasets/CRAFT_BIO, /home/john/dev/saber/datasets/BioNLP13CG_BIO
output_folder = /home/john/dev/saber/output
pretrained_model = 
pretrained_embeddings = /home/john/dev/saber/word_embeddings/wikipedia-pubmed-and-PMC-w2v.bin

[model]
word_embed_dim = 200
char_embed_dim = 30

[training]
optimizer = nadam
activation = relu
grad_norm = 1.0
learning_rate = 0.0
decay = 0.0
dropout_rate = 0.3, 0.3, 0.1
batch_size = 32
k_folds = 5
epochs = 50
criteria = exact

[advanced]
verbose = True
debug = False
tensorboard = False
save_all_weights = True
replace_rare_tokens = False
load_all_embeddings = False
fine_tune_word_embeddings = False
variational_dropout = True

